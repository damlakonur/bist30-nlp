{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuned_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQiKYjL3bZdM",
        "outputId": "4da67e4d-d6c8-437c-95af-69b369c0118a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piuaVNAvbg0l",
        "outputId": "c520aa89-bec3-4db9-d42b-5d381d8413d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('labels.csv')"
      ],
      "metadata": {
        "id": "jIrsF-r5b6WX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.concat([data.drop('label', axis=1), data['label'].str.get_dummies('#')], axis=1)\n",
        "labels.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Drz8StCOcPMU",
        "outputId": "77c9a5aa-f8a6-4228-fc77-bd91779bdb8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text  \\\n",
              "0  221221  Yƒ±l 2020  Bist100 tarafƒ±nda bilan√ßo d√∂nemi ve ...   \n",
              "1  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
              "2  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
              "3  221224  Kƒ±sa notlarƒ±mƒ± payla≈ümaya ba≈ülƒ±yorum.\\nBayramd...   \n",
              "4  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
              "\n",
              "                        date             user   rt   fav  followers  verified  \\\n",
              "0  2022-07-11 13:38:08+00:00     borsaparatic   17   180     220342     False   \n",
              "1  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
              "2  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
              "3  2022-07-09 19:44:59+00:00  ibrahim___ethem  134  1529     136873     False   \n",
              "4  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
              "\n",
              "   Ger√ßek  Negatif  N√∂tr  Pozitif  Yargƒ±  \n",
              "0       0        0     0        1      1  \n",
              "1       1        0     0        1      0  \n",
              "2       1        0     0        1      0  \n",
              "3       0        0     0        1      1  \n",
              "4       1        0     1        0      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-677668b3-49aa-4113-bfa2-dd0df9b9343e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>user</th>\n",
              "      <th>rt</th>\n",
              "      <th>fav</th>\n",
              "      <th>followers</th>\n",
              "      <th>verified</th>\n",
              "      <th>Ger√ßek</th>\n",
              "      <th>Negatif</th>\n",
              "      <th>N√∂tr</th>\n",
              "      <th>Pozitif</th>\n",
              "      <th>Yargƒ±</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>221221</td>\n",
              "      <td>Yƒ±l 2020  Bist100 tarafƒ±nda bilan√ßo d√∂nemi ve ...</td>\n",
              "      <td>2022-07-11 13:38:08+00:00</td>\n",
              "      <td>borsaparatic</td>\n",
              "      <td>17</td>\n",
              "      <td>180</td>\n",
              "      <td>220342</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>221222</td>\n",
              "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
              "      <td>2022-07-10 10:16:06+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>20</td>\n",
              "      <td>343</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>221223</td>\n",
              "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
              "      <td>2022-07-09 20:53:12+00:00</td>\n",
              "      <td>ASIM_YALCINKAYA</td>\n",
              "      <td>107</td>\n",
              "      <td>934</td>\n",
              "      <td>138759</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>221224</td>\n",
              "      <td>Kƒ±sa notlarƒ±mƒ± payla≈ümaya ba≈ülƒ±yorum.\\nBayramd...</td>\n",
              "      <td>2022-07-09 19:44:59+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>134</td>\n",
              "      <td>1529</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>221225</td>\n",
              "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
              "      <td>2022-07-09 07:01:24+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>23</td>\n",
              "      <td>379</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-677668b3-49aa-4113-bfa2-dd0df9b9343e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-677668b3-49aa-4113-bfa2-dd0df9b9343e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-677668b3-49aa-4113-bfa2-dd0df9b9343e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels[labels['Ger√ßek'] == 1]), len(labels[labels['Yargƒ±'] == 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HullGUSecS4i",
        "outputId": "25bf3414-cfb1-4065-9f0b-7eb98dd70f6a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(746, 639)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bin = labels.drop(['Pozitif', 'Negatif', 'N√∂tr', 'Yargƒ±'], axis=1)"
      ],
      "metadata": {
        "id": "e6EZj290cWh4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename column\n",
        "# 1 for fact, zero for opinion\n",
        "bin.rename(columns={'Ger√ßek': 'label'}, inplace=True)\n",
        "bin.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "DkTWRyugcbyl",
        "outputId": "6cd69728-7818-434e-9d7e-50fb92866fff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text  \\\n",
              "0  221221  Yƒ±l 2020  Bist100 tarafƒ±nda bilan√ßo d√∂nemi ve ...   \n",
              "1  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
              "2  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
              "3  221224  Kƒ±sa notlarƒ±mƒ± payla≈ümaya ba≈ülƒ±yorum.\\nBayramd...   \n",
              "4  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
              "\n",
              "                        date             user   rt   fav  followers  verified  \\\n",
              "0  2022-07-11 13:38:08+00:00     borsaparatic   17   180     220342     False   \n",
              "1  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
              "2  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
              "3  2022-07-09 19:44:59+00:00  ibrahim___ethem  134  1529     136873     False   \n",
              "4  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      1  \n",
              "2      1  \n",
              "3      0  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d938fac-e10f-4468-9ca8-ec27fa4be6d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>user</th>\n",
              "      <th>rt</th>\n",
              "      <th>fav</th>\n",
              "      <th>followers</th>\n",
              "      <th>verified</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>221221</td>\n",
              "      <td>Yƒ±l 2020  Bist100 tarafƒ±nda bilan√ßo d√∂nemi ve ...</td>\n",
              "      <td>2022-07-11 13:38:08+00:00</td>\n",
              "      <td>borsaparatic</td>\n",
              "      <td>17</td>\n",
              "      <td>180</td>\n",
              "      <td>220342</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>221222</td>\n",
              "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
              "      <td>2022-07-10 10:16:06+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>20</td>\n",
              "      <td>343</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>221223</td>\n",
              "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
              "      <td>2022-07-09 20:53:12+00:00</td>\n",
              "      <td>ASIM_YALCINKAYA</td>\n",
              "      <td>107</td>\n",
              "      <td>934</td>\n",
              "      <td>138759</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>221224</td>\n",
              "      <td>Kƒ±sa notlarƒ±mƒ± payla≈ümaya ba≈ülƒ±yorum.\\nBayramd...</td>\n",
              "      <td>2022-07-09 19:44:59+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>134</td>\n",
              "      <td>1529</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>221225</td>\n",
              "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
              "      <td>2022-07-09 07:01:24+00:00</td>\n",
              "      <td>ibrahim___ethem</td>\n",
              "      <td>23</td>\n",
              "      <td>379</td>\n",
              "      <td>136873</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d938fac-e10f-4468-9ca8-ec27fa4be6d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d938fac-e10f-4468-9ca8-ec27fa4be6d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d938fac-e10f-4468-9ca8-ec27fa4be6d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-uncased', do_lower_case=True)\n",
        "sentences = bin.text.values\n",
        "max_len = 250"
      ],
      "metadata": {
        "id": "-3mZ_UMucezu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = bin.groupby('label').apply(lambda x : x.sample(frac = 0.8))\n",
        "test = pd.concat([bin,training]).drop_duplicates(keep=False)\n",
        "\n",
        "print(\"Training: \", len(training))\n",
        "print(\"Test: \", len(test))\n",
        "\n",
        "training_texts = training.text.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO1whZ-hchb2",
        "outputId": "0e3dc593-58a6-4ded-cf61-4581307f2abb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  1120\n",
            "Test:  280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(training[training['label'] == 1]), len(training[training['label'] == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4xPYrGVcjnn",
        "outputId": "b4a7920c-10c2-4e9c-facf-24d83b8b1c6b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(597, 523)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test[test['label'] == 1]), len(test[test['label'] == 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0AYF5T_cwKn",
        "outputId": "2365f7f4-c313-49b1-d291-39d6c222c1a8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149, 131)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = training.label.values"
      ],
      "metadata": {
        "id": "NJojMB40c1-w"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in training_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = max_len,      \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(training_labels)\n",
        "\n",
        "print('Original: ', training_texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3uYZiJzc5Ov",
        "outputId": "1d0fc95c-3f0a-4585-df6a-4a3588e4db01"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  #petkm 8,50 yi alƒ±rsa saatlik macd tam sona gelecek.\n",
            "\n",
            "8,54 √º de alƒ±rsa net kƒ±rƒ±lƒ±m olur.\n",
            "Token IDs: tensor([    2,     7,  4286, 17762,    28,    16,  3603,  3553, 34205, 10925,\n",
            "         7270,  1020,  2795,  5187,  3658,    18,    28,    16,  7186,    63,\n",
            "         1961, 34205,  3542, 17459,  1982,  2544,    18,     3,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "number_of_categories = len(bin['label'].unique())\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dbmdz/bert-base-turkish-128k-uncased\",\n",
        "    num_labels = number_of_categories, \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "# \n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-e7oTSbc739",
        "outputId": "bcacc8ed-f41a-4d6b-dc94-a61aff425339"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5,\n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK6J10MbdAzW",
        "outputId": "b5a0f2bb-f555-4136-f509-23819237a843"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 1903\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 10 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        output = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels)\n",
        "        loss = output['loss']\n",
        "        logits = output['logits']\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training completed in {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sli1e1IdLK9",
        "outputId": "408d8829-fb53-4aec-c7fe-66da8ddc561b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Batch    10  of     35.    Elapsed: 0:00:13.\n",
            "Batch    20  of     35.    Elapsed: 0:00:27.\n",
            "Batch    30  of     35.    Elapsed: 0:00:40.\n",
            "Average training loss: 0.35\n",
            "Training epoch took: 0:00:47\n",
            "======== Epoch 2 / 4 ========\n",
            "Batch    10  of     35.    Elapsed: 0:00:14.\n",
            "Batch    20  of     35.    Elapsed: 0:00:29.\n",
            "Batch    30  of     35.    Elapsed: 0:00:43.\n",
            "Average training loss: 0.20\n",
            "Training epoch took: 0:00:50\n",
            "======== Epoch 3 / 4 ========\n",
            "Batch    10  of     35.    Elapsed: 0:00:14.\n",
            "Batch    20  of     35.    Elapsed: 0:00:28.\n",
            "Batch    30  of     35.    Elapsed: 0:00:42.\n",
            "Average training loss: 0.12\n",
            "Training epoch took: 0:00:49\n",
            "======== Epoch 4 / 4 ========\n",
            "Batch    10  of     35.    Elapsed: 0:00:14.\n",
            "Batch    20  of     35.    Elapsed: 0:00:29.\n",
            "Batch    30  of     35.    Elapsed: 0:00:43.\n",
            "Average training loss: 0.08\n",
            "Training epoch took: 0:00:50\n",
            "Training completed in 0:03:17 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "plt.plot(df_stats['Training Loss'], label=\"Training\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Z9k5eyRxdUkc",
        "outputId": "271a253f-3be4-45bc-fcf5-a4239a1190be"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "D3G32z5-htiK",
        "outputId": "efef5938-50a9-48cb-ad36-bff4a380be18"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Training Loss Training Time\n",
              "epoch                             \n",
              "1               0.35       0:00:47\n",
              "2               0.20       0:00:50\n",
              "3               0.12       0:00:49\n",
              "4               0.08       0:00:50"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fe96275-3fe1-4daf-940b-12b267aa9ac4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Training Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0:00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0:00:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0:00:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.08</td>\n",
              "      <td>0:00:50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fe96275-3fe1-4daf-940b-12b267aa9ac4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0fe96275-3fe1-4daf-940b-12b267aa9ac4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0fe96275-3fe1-4daf-940b-12b267aa9ac4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_texts = test.text.values\n",
        "test_labels = test.label.values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in test_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',   \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(test_labels)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmN4Kvdddh2h",
        "outputId": "18c16dc1-2260-46c1-d968-dfc5fdde7cc1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Prediction started on test data')\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('Prediction completed')\n",
        "\n",
        "prediction_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  prediction_set.append(pred_labels_i)\n",
        "\n",
        "prediction_scores = [item for sublist in prediction_set for item in sublist]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZPxpmR4dmPD",
        "outputId": "f88f5462-0ead-487a-a342-1f14d490977d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction started on test data\n",
            "Prediction completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['prediction'] = prediction_scores"
      ],
      "metadata": {
        "id": "h84abNw5iSzq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rmPm5xGMinEO",
        "outputId": "1dc560ca-fcc8-4dc0-ca73-000603f5e177"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                               text  \\\n",
              "2   221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
              "8   221229  Malum 3 site taramasƒ± CUMA\\nG√ú√áL√ú AL &amp; AL ...   \n",
              "18  221239  #ALKIM #ASELS #CMBTN #EGGUB #GUBRF #ISGSY #TCE...   \n",
              "19  221240  üö®HSBC, Ereƒüli ve Kardemir i√ßin tavsiyelerini k...   \n",
              "21  221242  Bug√ºn devam ettirdiƒüim alƒ±mlarla alƒ±m hisse ma...   \n",
              "\n",
              "                         date             user   rt   fav  followers  \\\n",
              "2   2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759   \n",
              "8   2022-07-07 19:16:07+00:00  ASIM_YALCINKAYA   10   132     138759   \n",
              "18  2022-07-07 07:33:40+00:00      Piri_KAPTAN    9   251     135214   \n",
              "19  2022-07-07 06:59:36+00:00      kursadbucak   10   350     156030   \n",
              "21  2022-07-06 19:05:20+00:00      kursadbucak   58  1089     156030   \n",
              "\n",
              "    verified  label  prediction  \n",
              "2      False      1           1  \n",
              "8      False      0           0  \n",
              "18     False      0           0  \n",
              "19     False      1           1  \n",
              "21     False      1           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a968bbf-e7ff-45c7-a3f9-f5eca93e02c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>user</th>\n",
              "      <th>rt</th>\n",
              "      <th>fav</th>\n",
              "      <th>followers</th>\n",
              "      <th>verified</th>\n",
              "      <th>label</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>221223</td>\n",
              "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
              "      <td>2022-07-09 20:53:12+00:00</td>\n",
              "      <td>ASIM_YALCINKAYA</td>\n",
              "      <td>107</td>\n",
              "      <td>934</td>\n",
              "      <td>138759</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>221229</td>\n",
              "      <td>Malum 3 site taramasƒ± CUMA\\nG√ú√áL√ú AL &amp;amp; AL ...</td>\n",
              "      <td>2022-07-07 19:16:07+00:00</td>\n",
              "      <td>ASIM_YALCINKAYA</td>\n",
              "      <td>10</td>\n",
              "      <td>132</td>\n",
              "      <td>138759</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>221239</td>\n",
              "      <td>#ALKIM #ASELS #CMBTN #EGGUB #GUBRF #ISGSY #TCE...</td>\n",
              "      <td>2022-07-07 07:33:40+00:00</td>\n",
              "      <td>Piri_KAPTAN</td>\n",
              "      <td>9</td>\n",
              "      <td>251</td>\n",
              "      <td>135214</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>221240</td>\n",
              "      <td>üö®HSBC, Ereƒüli ve Kardemir i√ßin tavsiyelerini k...</td>\n",
              "      <td>2022-07-07 06:59:36+00:00</td>\n",
              "      <td>kursadbucak</td>\n",
              "      <td>10</td>\n",
              "      <td>350</td>\n",
              "      <td>156030</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>221242</td>\n",
              "      <td>Bug√ºn devam ettirdiƒüim alƒ±mlarla alƒ±m hisse ma...</td>\n",
              "      <td>2022-07-06 19:05:20+00:00</td>\n",
              "      <td>kursadbucak</td>\n",
              "      <td>58</td>\n",
              "      <td>1089</td>\n",
              "      <td>156030</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a968bbf-e7ff-45c7-a3f9-f5eca93e02c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a968bbf-e7ff-45c7-a3f9-f5eca93e02c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a968bbf-e7ff-45c7-a3f9-f5eca93e02c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_score = f1_score(test_labels, prediction_scores, average='macro')\n",
        "precision = precision_score(test_labels, prediction_scores, average='macro')\n",
        "recall = recall_score(test_labels, prediction_scores, average='macro')\n",
        "\n",
        "print(\"F-Score: \", f_score)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"Precision: \", precision)\n",
        "\n",
        "report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))\n",
        "report = report.rename(columns={'0':'Yargƒ±',\n",
        "                          '1':'Ger√ßek'})\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzMI7Lrudpiz",
        "outputId": "f46d7ede-59c4-4780-cd2a-20cd757f43cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Score:  0.9065660454848812\n",
            "Recall:  0.905835339925201\n",
            "Precision:  0.9076733055426895\n",
            "                Yargƒ±      Ger√ßek  accuracy   macro avg  weighted avg\n",
            "precision    0.913386    0.901961  0.907143    0.907673      0.907306\n",
            "recall       0.885496    0.926174  0.907143    0.905835      0.907143\n",
            "f1-score     0.899225    0.913907  0.907143    0.906566      0.907038\n",
            "support    131.000000  149.000000  0.907143  280.000000    280.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HjhnUKghW7K",
        "outputId": "d5b20c2c-13a8-411a-a84a-3ea08cb680e4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to ./model_save/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/tokenizer_config.json',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/vocab.txt',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',   \n",
        "                   )"
      ],
      "metadata": {
        "id": "_xFupc92qsA1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = '#ASELS hissesi kƒ±sa zaman i√ßinde y√ºkseli≈üe ge√ßecek. ytd'\n",
        "\n",
        "# We need Token IDs and Attention Mask for inference on the new sentence\n",
        "test_ids = []\n",
        "test_attention_mask = []\n",
        "\n",
        "# Apply the tokenizer\n",
        "encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "# Extract IDs and Attention Mask\n",
        "test_ids.append(encoding['input_ids'])\n",
        "test_attention_mask.append(encoding['attention_mask'])\n",
        "test_ids = torch.cat(test_ids, dim = 0)\n",
        "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "# Forward pass, calculate logit predictions\n",
        "with torch.no_grad():\n",
        "  output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "prediction = 'Ger√ßek' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Yargƒ±'\n",
        "\n",
        "print('Input Sentence: ', new_sentence)\n",
        "print('Predicted Class: ', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "976mjyo4qI2q",
        "outputId": "a7565ada-04b7-4000-dc34-718297a0d1be"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  #ASELS hissesi kƒ±sa zaman i√ßinde y√ºkseli≈üe ge√ßecek. ytd\n",
            "Predicted Class:  Yargƒ±\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f7GDSMe3qgMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}